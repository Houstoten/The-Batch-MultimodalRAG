{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kaggle import api \n",
    "from dotenv import load_dotenv\n",
    "from app.pipelines.workflow import setup_preloaded_chroma\n",
    "from app.modules.qa_chain_composer.utils import create_multi_modal_query\n",
    "\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ['ROOT_DIR'] = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./data/doc_dataset\"):\n",
    "    api.dataset_download_files('ivanhusarov/the-batch-articles-initial', path='./data/doc_dataset', unzip=True)\n",
    "if not os.path.exists(\"./data/image_descriptions_dataset\"):\n",
    "    api.dataset_download_files('ivanhusarov/the-batch-articles-image-descriptions', path='./data/image_descriptions_dataset', unzip=True)\n",
    "if not os.path.exists(\"./chroma_preloaded\"):\n",
    "    api.dataset_download_files('ivanhusarov/the-batch-rag-chroma', path='chroma_preloaded', unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = setup_preloaded_chroma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model='gpt-4o', temperature=0)\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = \"\"\"Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know':\n",
    "\n",
    "### CONTEXT\n",
    "{context}\n",
    "\n",
    "### QUESTION\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template, \n",
    "    input_variables=[\n",
    "        'context', \n",
    "        'question',\n",
    "    ]\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                    retriever=compression_retriever,     \n",
    "                                    return_source_documents=True,\n",
    "                                    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"Explain the misclassification in AI, mentioning the bias in ML models?\"\n",
    "# query = create_multi_modal_query(question=question, image=None)\n",
    "# answer = qa_chain.invoke({\"query\": query})\n",
    "# answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What was the surprising outcome of the auction for the painting created by the AI-powered robot Ai-Da?',\n",
       " 'answer': 'A painting of mathematician Alan Turing created by the AI-powered robot Ai-Da sold at Sotheby’s for $1.1 million, far exceeding initial estimates.'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "from datasets import Dataset\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define the schema for output\n",
    "class QAFormat(BaseModel):\n",
    "    question: str = Field(default=None, description=\"Generated question about the context\")\n",
    "    answer: str = Field(default=None, description=\"Generated answer on question from the context\")\n",
    "\n",
    "qa_template = \"\"\"\\\n",
    "You are a University Professor creating a test for advanced students. Create a question that is specific to the context. Avoid creating generic or general questions. Don't say I don't know.\n",
    "Consider the following context: {context}\n",
    "\n",
    "\n",
    "Return your response as a JSON object with the following structure:\n",
    "{{\n",
    "    \"question\": a question about the context.\n",
    "    \"answer\": an answer from the context.\n",
    "}}\n",
    "\n",
    "Always return your response as a JSON object.\n",
    "\"\"\"\n",
    "\n",
    "full_doc_list = vector_store.get()[\"documents\"]\n",
    "context = full_doc_list[0]\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=QAFormat)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=qa_template,\n",
    "    input_variables=[\"context\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "full_doc_list = vector_store.get()[\"documents\"]\n",
    "processed_sample = sample(full_doc_list, 20)\n",
    "\n",
    "synthetic_questions = []\n",
    "synthetic_ground_truths = []\n",
    "contexts = []\n",
    "\n",
    "for sample_entry in processed_sample:\n",
    "    result = chain.invoke({\"context\": sample_entry})\n",
    "    synthetic_ground_truths.append(result[\"answer\"])\n",
    "    synthetic_questions.append(result[\"question\"])\n",
    "    contexts.append(sample_entry)\n",
    "\n",
    "questions = synthetic_questions\n",
    "ground_truths = synthetic_ground_truths\n",
    "# references = processed_sample\n",
    "# answers = []\n",
    "# contexts = []\n",
    "\n",
    "# # # Inference\n",
    "# # for query in questions:\n",
    "# #     qa_response = qa_chain.invoke({\"query\": query})\n",
    "# #     answers.append(qa_response['result'])\n",
    "# #     contexts.append([docs.page_content for docs in qa_response['source_documents']])\n",
    "\n",
    "# # To dict\n",
    "data = {\n",
    "    \"question\": questions,\n",
    "    # \"answer\": answers,\n",
    "    # \"reference\": references,\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truth\": ground_truths\n",
    "}\n",
    "\n",
    "# # Convert dict to dataset\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the key advancement in the new architecture that allows it to outperform a fully trained AlexNet model with only a few labeled samples?',\n",
       " 'contexts': 'The power of deep learning is blunted in domains where labeled training data is scarce.But that may be changing, thanks to a new architecture that recognizes images with high accuracy based on few labeled samples.What’s new: Researchers devised a network that, given a small number of labeled images, can learn enough from unlabeled images to outperform a fully trained AlexNet model.',\n",
       " 'ground_truth': 'The new architecture can learn enough from unlabeled images to achieve high accuracy, even with a small number of labeled images.'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dspy.teleprompt import BootstrapFewShot\n",
    "from typing import List, Dict\n",
    "from dspy.teleprompt import BootstrapFewShot\n",
    "from dspy.evaluate.evaluate import Evaluate\n",
    "from dspy.evaluate import SemanticF1, f1_score, DecompositionalSemanticRecallPrecision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy import OpenAI\n",
    "\n",
    "model = OpenAI(model='gpt-4o', max_tokens=400)\n",
    "dspy.settings.configure(lm=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class QuestionAnswerSignature(dspy.Signature):\n",
    "#     \"\"\"Retrieve the most relevant answer to a given question using available context.\"\"\"\n",
    "#     question = dspy.InputField()\n",
    "#     context = dspy.InputField(desc=\"Supporting information to answer the question\")\n",
    "#     answer = dspy.OutputField(desc=\"Concise and accurate answer to the question\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "STARTING_SYSTEM_PROMPT = \"\"\"Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know':\n",
    "\n",
    "### CONTEXT\n",
    "{context}\n",
    "\n",
    "### QUESTION\n",
    "Question: {question}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_signature = dspy.Signature(\"question, context -> answer\")\n",
    "QA_program = dspy.ChainOfThought(qa_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewDecompositionalSemanticRecallPrecision(DecompositionalSemanticRecallPrecision):\n",
    "    recall: float = dspy.OutputField(desc=\"fraction (out of 1.0) of ground truth covered by the system response. Please, return only float number without text and explanation.\")\n",
    "    precision: float = dspy.OutputField(desc=\"fraction (out of 1.0) of system response covered by the ground truth. Please, return only float number without text and explanation.\")\n",
    "\n",
    "class NewSemanticF1(SemanticF1):\n",
    "    def __init__(self, threshold=0.66, decompositional=False):\n",
    "        self.threshold = threshold\n",
    "        self.module = dspy.ChainOfThought(NewDecompositionalSemanticRecallPrecision)\n",
    "\n",
    "\n",
    "    def forward(self, example, pred, trace=None):\n",
    "        scores = self.module(question=example[\"question\"], ground_truth=example[\"response\"], system_response=pred[\"response\"])\n",
    "        score = f1_score(float(scores.precision), float(scores.recall))\n",
    "\n",
    "        return score if trace is None else score >= self.threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt = \"\"\"You are an advanced retrieval and generation assistant, trained to provide **detailed, accurate, and contextually relevant answers**. Leverage the provided **context**, user inputs, and embedded memory to construct logically sound and comprehensive responses.\n",
    "\n",
    "### Task:\n",
    "1. **Understand the Query**: Analyze user inputs to discern intent and determine the best retrieval and synthesis strategy.\n",
    "2. **Employ Chain of Thought (CoT)**: Break down reasoning step by step to ensure depth, accuracy, and coherence in the response.\n",
    "3. **Leverage Context**: Integrate contextual information with retrieved knowledge to ensure the answer aligns with the user's requirements.\n",
    "4. **Generate Outputs Dynamically**: Tailor the response to the input format (text, image description, or both).\n",
    "\n",
    "### Input Schema:\n",
    "- **Context**: {context}\n",
    "- **Text Input**: {text_input} (optional)\n",
    "- **Image Description**: {image_description} (optional)\n",
    "\n",
    "### Instructions by Scenario:\n",
    "1. **Only Text Provided**:\n",
    "   - Focus exclusively on the text input.\n",
    "   - Extract key details from the context to complement the analysis.\n",
    "2. **Only Image Description Provided**:\n",
    "   - Analyze the described image thoroughly.\n",
    "   - Use the context to enhance the interpretation of the visual elements.\n",
    "3. **Both Text and Image Description Provided**:\n",
    "   - Synthesize information from both inputs for a cohesive and nuanced response.\n",
    "\n",
    "### Chain of Thought (CoT) Reasoning:\n",
    "- **Step 1**: Analyze all inputs to determine the user’s intent.\n",
    "- **Step 2**: Identify the required knowledge domains and retrieve relevant information.\n",
    "- **Step 3**: Verify the sufficiency of retrieved information against the context and inputs.\n",
    "- **Step 4**: Formulate a detailed, coherent, and accurate response.\n",
    "\n",
    "### Final Response:\n",
    "- Structure: Ensure the output is clear, precise, and aligned with user intent.\n",
    "- Depth: Provide additional insights or actionable recommendations where appropriate.\n",
    "- Clarity: Avoid unnecessary jargon while maintaining technical accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Queries for Different Scenarios\n",
    "\n",
    "#### **Scenario 1: User Inputs Only Text**\n",
    "**Context**: Recent advancements in quantum computing.\n",
    "**Text Input**: \"Explain the impact of qubits on computational speed.\"\n",
    "**Response**: (Use CoT reasoning to address the question in depth, referencing the context on quantum computing.)\n",
    "\n",
    "#### **Scenario 2: User Inputs Only Image Description**\n",
    "**Context**: Environmental conservation initiatives.\n",
    "**Image Description**: \"A detailed image of a river surrounded by deforested land.\"\n",
    "**Response**: (Analyze the environmental implications using the context and interpret the visual information.)\n",
    "\n",
    "#### **Scenario 3: User Inputs Both Text and Image Description**\n",
    "**Context**: Climate change and renewable energy.\n",
    "**Text Input**: \"What are the benefits of solar panels in urban areas?\"\n",
    "**Image Description**: \"An image of a rooftop covered with solar panels.\"\n",
    "**Response**: (Synthesize both inputs to create a comprehensive answer about urban solar panel benefits.)\n",
    "\n",
    "---\n",
    "\n",
    "Return your response as a JSON object with the following structure:\n",
    "{{\n",
    "    \"answer\": an answer from the context.\n",
    "}}\n",
    "\n",
    "Always return your response as a JSON object.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAFormat_final(BaseModel):\n",
    "    answer: str = Field(default=None, description=\"Generated answer on question from the context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = JsonOutputParser(pydantic_object=QAFormat_final)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=rag_prompt,\n",
    "    input_variables=[\"context\", \"text_input\", \"image_description\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'The Allen Institute for AI offers CoViz, an interactive network that visualizes relationships among concepts present in the COVID-19 Open Research Dataset.'}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"context\": dataset[0]['contexts'], \"text_input\": dataset[0]['question'], \"image_description\": \"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:02<00:00,  6.11s/it]\n"
     ]
    }
   ],
   "source": [
    "metric = NewSemanticF1()\n",
    "\n",
    "f1_metric_arr = []\n",
    "for entry in tqdm(dataset):\n",
    "\n",
    "    response = chain.invoke({\"context\": entry['contexts'], \"text_input\": entry['question'], \"image_description\": \"\"})\n",
    "    # response = QA_program(question=entry['question'], context=entry['contexts'])\n",
    "    f1_metric_arr.append(metric({\"question\": entry['question'], \"response\": entry['ground_truth']}, {\"response\": response['answer']} ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7556905019294613"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.average(f1_metric_arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a9ec9383b00351bef21759d22463dd34ead97c37b3b59e96735d5b9642fa602"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
